[
{
	"uri": "https://geometry-of-relativity.net/intro/",
	"title": "Intro to Geometric Algebra",
	"tags": [],
	"description": "",
	"content": "Geometric Algebra is a line of Mathematics which treats vectors as first-class citizens rather than arrows defined by coordinates on a specific coordinate system.\nFor a flat Euclidean space, the most interesting aspects of Geometric Algebra can be understood from a single rule that the square of the magnitude of any vector \\( \\v{a} \\) is equal to the square of the vector itself.\nWith this one rule we can introduce addition and multiplication of two-dimensional vectors, as well as a simple rotation.\nOne rule The square of the magnitude of an arbitrary vector \\( \\v{a} \\) is equal to the square of the vector itself:\n\\[ |\\v{a}|^2 = \\v{a}^2 \\]\nThis is similar to the more familiar dot-product of a vector with itself defining the Euclidean magnitude-squared of a vector, but as we will see, much more powerful.\nNormal basis vectors Starting with a single dimension, we define a normal basis vector \\( \\v{x} \\) (bold will indicate a vector) as having a magnitude of one:\n\\[ \\v{x}^2 = 1\\]\nAny other vector \\( \\v{a} \\) within this single dimension can be represented as a scalar multiple of \\( \\v{x} \\), such as \\( 5\\v{x} \\) or \\( -3.141579\\v{x} \\) or, more generally,\n\\[\\v{a} = \\vcOne{a}\\]\nwhere \\( a_x \\) is a real number (\\( a_x \\in \\reals \\)).\nEvaluating the magnitude-squared of \\( \\v{a} \\) results in:\n\\[ \\v{a}^2 = (\\vcOne{a})^2 = \\vcOne{a}\\vcOne{a} = a_x^2\\v{x}^2 = a_x^2 = |a_x|^2 \\]\nas we would expect.\nOrthogonal basis vectors To describe vectors in two dimensions we need to add a second basis vector \\( \\v{y} \\) with the similar property of unit-magnitude, so when multiplied by itself,\n\\[ \\v{y}^2 = 1 \\]\nWe also need to ensure that this second vector \\( \\v{y} \\) is orthogonal to \\( \\v{x} \\), though we don't yet know how to represent this orthogonality. Nonetheless, by defining a general two-dimensional vector \\( \\v{a} \\) as\n\\[ \\v{a} = \\vcTwo{a} \\]\nwe can derive orthogonality by using the rule that the magnitude-squared of any vector is equal to the vector squared, to evaluate that:\n$$ \\begin{aligned} \\v{a}^2 \u0026= (\\vcTwo{a})(\\vcTwo{a}) \\\\ \u0026= a_x\\v{x}a_x\\v{x} + a_x\\v{x}a_y\\v{y} + a_y\\v{y}a_x\\v{x} + a_y\\v{y}a_y\\v{y} \\\\ \u0026= a_x^2 + a_y^2 + a_x a_y(\\v{xy} + \\v{yx}) \\\\ \u0026= a_x^2 + a_y^2 + 2a_x a_y(\\frac{1}{2}(\\v{xy} + \\v{yx})) \\end{aligned} $$ So the square of the magnitude of the vector, \\( |\\v{a}|^2 \\), will only equal the square of the vector if \\( \\v{xy}+ \\v{yx} \\) is also a scalar number. If you are familiar with the Law of cosines you may recognise the formula and in particular that \\( \\frac{1}{2}(\\v{xy} + \\v{yx}) \\) is likely to be \\( \\cos(\\theta) \\) where \\( \\theta \\) is the angle between the two vectors. For now we will simply note that if,\n\\[ \\v{xy} = -\\v{yx} \\]\nthen,\n\\[ \\v{a}^2 = a_x^2 + a_y^2 \\]\nwhich is exactly what we are used to - Pythagoras' theorem. So for now let us make the tentative definition of orthogonal vectors: orthogonal vectors anti-commute. For any two vectors \\( \\v{x} \\) and \\( \\v{y} \\), the vectors are orthogonal if they anti-commute so that \\( \\v{xy} = -\\v{{yx}} \\).\nAdding two vectors The algebraic addition of two arbitrary two-dimensional vectors is straight forward,\n\\[ \\begin{aligned} \\v{a} + \\v{b} \u0026= (\\vcTwo{a}) + (\\vcTwo{b}) \\\\ \u0026= (a_x + b_x)\\v{x} + (a_y + b_y)\\v{y} \\\\ \u0026= \\v{b} + \\v{a} \\end{aligned} \\]\nThe order in which the vectors are added is not relevant which means that vectors are associative under addition (and subtraction), as \\(\\v{a} + \\v{b} = \\v{b} + \\v{a}\\).\nMagnitude squared of two vectors See if you can calculate the magnitude-squared of $$\\v{a} + \\v{b}$$  Solution \n\\[ \\begin{aligned} (\\v{a} + \\v{b})^2 \u0026= (\\v{a} + \\v{b})(\\v{a} + \\v{b}) \\\\ \u0026= \\v{a}^2 + \\v{ab} + \\v{ba} + \\v{b}^2 \\end{aligned} \\]\nWe can't yet go any further as we haven't yet defined what a multiple of two general vectors means. But soon we'll see that our solution here is actually the law of cosines.\n  Multiplying two vectors The geometric product of two vectors may be a little less familiar:\n\\[ \\begin{aligned} \\v{a} \\v{b} \u0026= (\\vcTwo{a})(\\vcTwo{b}) \\\\ \u0026= a_x\\v{x}b_x\\v{x} + a_x\\v{x}b_y\\v{y} + a_y\\v{y}b_x\\v{x} + a_y\\v{y}b_y\\v{y} \\\\ \u0026= a_x b_x + a_y b_y + a_x b_y \\v{xy} + a_y b_x \\v{yx} \\\\ \u0026= a_x b_x + a_y b_y + \\v{xy}(a_x b_y - a_y b_x) \\\\ \\end{aligned} \\]\nWhile the reverse is,\n\\[ \\v{ba} = a_x b_x + a_y b_y - \\v{xy}(a_x b_y - a_y b_x) \\]\nThe general vector product is not commutative, as \\(\\v{ab} \\ne \\v{ba}\\) nor is it anti-commutative, as \\(\\v{ab} \\ne -\\v{ba}\\). If you are familiar with the traditional vector dot and cross products, you may recognise \\(\\v{ab}\\) as some combination of both, but what is relevant is that it is a combination of a scalar (a pure number) and a bi-vector (the \\(\\v{xy}\\) term, which is not scalar, nor is it a vector, more on this later).\nBased on the above, we can add or subtract the two products to end up with either a scalar or bivector respectively:\n\\[ \\begin{aligned} \\frac{1}{2}(\\v{ab} + \\v{ba}) \u0026= (a_x b_x + a_y b_y) \\\\ \\frac{1}{2}(\\v{ab} - \\v{ba}) \u0026= \\v{xy}(a_x b_y - a_y b_x) \\end{aligned} \\]\nOf particular interest is that the bivector \\(\\v{xy}\\) has the property that:\n\\[ (\\v{xy})^2 = \\v{xyxy} = -\\v{xyyx} = -1 \\]\nRotating vectors in two-dimensions It is worth noting briefly that the product of two vectors, \\(\\v{ab}\\) appears to define a scale and rotation. We will look at this in more detail later, but we can already see that if \\(\\v{a}\\) and \\(\\v{b}\\) are perpendicular and have a magnitude of 1:\n\\[ \\begin{aligned} \\v{a} \u0026= \\v{x} \\\\ \\v{b} \u0026= \\v{y} \\end{aligned} \\]\nso that\n\\[ \\v{ab} = \\v{xy}\\]\nthen this vector product will rotate any other two-dimensional vector \\(\\v{c}\\). To simplify things, let's multiply the unit vector \\(\\v{x}\\):\n\\[ \\begin{aligned} \\v{abx} \u0026= \\v{xyx} \\\\ \u0026= -\\v{xxy} \\\\ \u0026= -\\v{y} \\\\ \\v{xy}(-\\v{y}) \u0026= -\\v{x} \\\\ \\v{xy}(-\\v{x}) \u0026= -\\v{xyx} \\\\ \u0026= \\v{xxy} \\\\ \u0026= \\v{y} \\\\ \\v{xy(y)} \u0026= \\v{x} \\\\ \\end{aligned} \\]\nBut we will first need to understand Euler's formula to appreciate this rotation in its general form.\n"
},
{
	"uri": "https://geometry-of-relativity.net/",
	"title": "The Geometry of Relativity",
	"tags": [],
	"description": "",
	"content": "The Geometry of Relativity Both relativity and quantum mechanics are often hard to grasp for our minds because they describe physical situations beyond the extremes of our every-day experience.\nThe mathematical tools that we use to describe Special Relativity can be marvelously helpful, but may also unintentionally hide from view a geometric understanding of the Lorentz transformation.\nGeometric Algebra is a relatively recent branch of Mathematics which simplifies many physical processes involving vectors. This site is my attempt to understand and communicate the beauty of geometric algebra as simply as possible while having fun investigating a derivation of the Lorentz transformation as a geometric rotation in space-time, as well as looking at the implications of that geometry. If you are after a formal introduction to Geometric Algebra there are many great resources linked from the BiVector.net docs page.\n Intro to Geometric Algebra  Geometric Algebra is a line of Mathematics which treats vectors as first-class citizens rather than arrows defined by coordinates on a specific coordinate system.\nFor a flat Euclidean space, the most interesting aspects of Geometric Algebra can be understood from a single rule that the square of the magnitude of any vector \\( \\v{a} \\) is equal to the square of the vector itself.\nWith this one rule we can introduce addition and multiplication of two-dimensional vectors, as well as a simple rotation.\n Rotations in Space  At the end of the previous section we saw that we can rotate an arbitrary vector by 90 degrees, \\(\\frac{\\pi}{2}\\) radians, simply by multiplying that vector by \\(\\mathbf{xy}\\). In this section we will see that this is because \\(\\mathbf{xy} = e^{\\mathbf{xy}\\frac{\\pi}{2}}\\) and that more generally we can rotate the same vector by any arbitrary angle \\(\\theta\\) simply by multiplying by \\(e^{\\mathbf{xy}\\theta}\\).\nBut to get to that point we'll first need to take a closer look at Euler's formula and its application to geometric algebra - where we no longer need to define the imaginary unit \\(i\\).\n Rotations in Space-Time  Building on rotations using geometric algebra, in this section we define a two-dimensional space-time multivector and then investigate a rotation in space as well as a rotation in space-time, the result of which is equivalent to the Lorentz transformation.\n "
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/",
	"title": "Rotations in Space",
	"tags": [],
	"description": "",
	"content": "At the end of the previous section we saw that we can rotate an arbitrary vector by 90 degrees, \\(\\frac{\\pi}{2}\\) radians, simply by multiplying that vector by \\(\\mathbf{xy}\\). In this section we will see that this is because \\(\\mathbf{xy} = e^{\\mathbf{xy}\\frac{\\pi}{2}}\\) and that more generally we can rotate the same vector by any arbitrary angle \\(\\theta\\) simply by multiplying by \\(e^{\\mathbf{xy}\\theta}\\).\nBut to get to that point we'll first need to take a closer look at Euler's formula and its application to geometric algebra - where we no longer need to define the imaginary unit \\(i\\).\n Euler\u0026#39;s Formula  Euler's formula, the most remarkable formula in mathematics, according to Richard Feynman and many others, states that for any real number \\(x\\),\n\\[ e^{ix} = \\cos x + i \\sin x \\]\nIn this section we will derive Euler's formula to get a better understanding of how Euler's formula can be used to describe rotations in space and time, eventually including the Lorentz transformation.\n Euler\u0026#39;s Formula and Geometric Algebra  While working to understand and derive Euler's formula we introduced an imaginary unit \\(i\\) with the property that \\(i^2 = -1\\). But we've already seen that the product of the two basis vectors has this same property in that \\( (\\v{xy})^2 = -1 \\), though it differs in other properties as we shall see.\nIn this section we will investigate the properties of various rotations in 2D planes using Euler's formula and Geometric Algebra in two dimensional space.\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-spacetime/a-vector-in-spacetime/",
	"title": "A (para)Vector of space and time",
	"tags": [],
	"description": "",
	"content": "So far we have considered operations on vectors only - linear combinations of the two basis vectors, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Here we will consider a more general combination of a vector together with a scalar component \\(a_0\\) to form a something called a paravector - a scalar value plus a vector value. Furthermore, we will see that this scalar component results in a amplitude-squared metric matching the metric of space-time in Special Relativity. Finally, we will see that we can rotate these space-time paravectors in space without affecting the temporal component, corresponding to changing our frame of reference within a single inertial frame.\nLet's start by defining an arbitrary two-dimensional paravector as a vector plus a scalar component:\n\\[ \\v{A} = a_0 + \\v{a} = a_0 + \\vcTwo{a}\\]\nNote that a more general multivector in two dimensions would additionally have a term in \\(\\v{xy}\\). We are limiting the discussion here to a paravector only so we can gain some insight from the two dimensional case before we examine the three dimensional case more generically.\nThe amplitude-squared of a 2D paravector Multiplying this paravector by itself, we see:\n\\[ \\begin{aligned} \\mathbf{A}^2 \u0026= (a_0 + \\v{a})(a_0 + \\v{a}) \\\\ \u0026= a_0^2 + \\v{a}a_0 + a_0\\v{a} + \\v{a}^2 \\\\ \u0026= a_0^{2} + \\mathbf{a}^2 + 2a_0\\v{a} \\end{aligned} \\]\nwhich is not a scalar at all. So, unlike the case of a vector where the amplitude squared is just the square of the vector \\(|\\v{a}|^2 = \\v{a}^2 \\), for a paravector \\( |\\v{A}|^2 \\ne \\v{A}^2 \\).\nWe need to find a conjugate of \\(\\v{A}\\), written \\(\\c{\\v{A}}\\), such that \\(|\\v{A}|^2 = \\v{A}\\c{\\v{A}} \\) results in a scalar value for a paravector while also simplifying, when the scalar component of \\(\\v{A}\\) is zero, to the familiar magnitude of a vector.\nAmplitude squared of a paravector See if you can show that the only possibility for such a conjugate of $$\\v{A}$$ is: $$ \\c{\\v{A}} = a_0 - \\v{a} $$ so that the ampitude squared is the scalar value $$ \\v{A}\\c{\\v{A}} = a_0^2 - \\v{a}^2 $$ As a hint to get started, if we instead assume that the conjugate is a general linear function of the parts: $$\\c{\\v{A}} = f(\\v{A}) = s_0 a_0 + s_a\\v{a}$$ we can then solve to find the values of the real numbers that result in a scalar value. Evaluating,\n\\[ \\begin{aligned} \\v{A}f(\\v{A}) \u0026= (\\v{a} + a_0)(s_a\\v{a} + s_0 a_0) \\\\ \u0026= s_a\\v{a}^2 + s_0 a_0 \\v{a} + a_0 s_a\\v{a} + s_0 a_0^2 \\\\ \u0026= s_a\\v{a}^2 + a_0 \\v{a}(s_0 + s_a) + s_0 a_0^2 \\end{aligned} \\]\nThis result will be a scalar value if and only if \\(s_0 + s_a = 0\\). Our further requirement that for a plain vector \\(|\\v{a}|^2 = |\\v{a}^2|\\) means that for the special case of \\(\\v{A} = \\v{a} + 0\\) we have:\n\\[\\v{A}f(\\v{A}) = (\\v{a} + 0)(s_a\\v{a} + s_0 0) = s_a\\v{a}^2\\]\nSo we are further restricted to \\(s_a = \\pm 1\\) and therefore \\(s_0 = \\mp 1\\). We will stick to convention for now (and see later on why the convention is useful) and choose: \\(s_a = -1\\) and \\(s_0 = 1\\) so that\n\\[ f(\\v{A}) = \\c{\\v{A}} = a_0 - \\v{a} \\]\n.\n  The definition of the conjugate, \\( \\c{\\v{A}} = a_0 - \\v{a} \\) gives us a metric for the amplitude squared of a scalar plus vector so that for any paravector \\(\\v{A} = a_0 + \\v{a}\\),\n\\[ |\\v{A}|^2 = \\v{A}\\c{\\v{A}} = a_0^2 - \\v{a}^2 \\]\nThe Clifford Conjugation The conjugation we have found here is called the Clifford conjugation. For a more general and formal definition of the Clifford Conjugation as well as a derivation of the amplitude squared for a multivector, see Exploring the origin of Minkowski spacetime.\nFor our purposes here, it is enough to understand that the Clifford conjugation of any value of the geometric algebra is found by reversing all geometric products and negating all vectors. Let's see some examples.\nConjugating vectors As we've already seen, if \\(\\v{A} = a_0 + \\v{a}\\), then\n\\[ \\c{\\v{A}} = a_0 - \\v{a}\\]\nIn this case, there are no products of vectors, so it is only the negation of vectors that is relevant. Note that we get the same result when represeting \\(\\v{a}\\) with respect to a specific orthonormal basis, so if \\(\\v{A} = a_0 + \\vcTwo{a}\\), then\n\\[ \\c{\\v{A}} = a_0 - a_x \\v{x} - a_y\\v{y} = a_0 - \\v{a} \\]\nConjugating products of vectors If a rotor is defined by the product of two normal vectors, \\(\\v{b}\\) and \\(\\v{c}\\), \\( \\v{R} = \\v{bc} \\), then\n\\[ \\c{\\v{R}} = \\c{\\v{bc}} = \\c{\\v{c}}\\c{\\v{b}} = (\\v{-c})(\\v{-b}) = \\v{cb} \\]\nSo in this case it is reversing the geometric product which is significant, while the negation of vectors cancels itself (in this case where there are an even number of vectors). Note that we get the same result if we instead represent the rotor as an exponent,\n\\[\\v{R} = \\v{bc} = \\cos(\\theta) + \\v{xy}\\sin(\\theta) = e^{\\v{xy}\\theta} \\]\nwhere \\(\\theta\\) is the angle between the two normal vectors, then\n\\[ \\c{\\v{R}} = e^{\\v{yx}\\theta} = e^{-\\v{xy}\\theta} \\]\nConjugating arbitrary terms If we define another paravector (or arbitrary multi-vector), \\(\\v{B}\\) and use it to define a product of three terms, \\(\\v{BAR}\\), then the Clifford conjugate of that product is the reverse product of the individual conjugations:\n\\[ \\c{\\v{BAR}} = \\c{\\v{R}}\\c{\\v{A}}\\c{\\v{B}} \\]\nThis is technically known as an anti-automorphism and it ensures that the Clifford conjugate can always be applied a second time to get back the original value:\n\\[ \\c{(\\c{\\v{R}}\\c{\\v{A}}\\c{\\v{B}})} = \\v{BAR} \\]\nwhich is also known more technically as an involution.\nNormalised paravectors An arbitrary paravector \\(\\v{A}\\) is normal if\n\\[|\\v{A}|^2 = \\v{A}\\c{\\v{A}} = a_0^2 - \\v{a}^2 = 1\\]\nNoting that this is similar to the trigonometric identity\n\\[\\cosh^2\\phi - \\sinh^2\\phi = 1\\]\nallows us to choose a normalized vector \\(\\v{a}\\) and a value of \\(\\phi\\) such that\n\\[ \\v{A} = \\cosh \\phi + \\v{a}\\sinh \\phi \\]\nWe can now verify that \\(\\v{A}\\) is normal as long as \\(\\v{a}\\) is normal by expanding:\n\\[ \\begin{aligned} \\v{A\\c{A}} \u0026= (\\cosh \\phi + \\v{a}\\sinh \\phi)(\\cosh \\phi - \\v{a}\\sinh \\phi) \\\\ \u0026= \\cosh^2\\phi - \\v{a}\\cosh\\phi\\sinh\\phi + \\v{a}\\cosh\\phi\\sinh\\phi - \\v{a}^2\\sinh^2\\phi \\\\ \u0026= \\cosh^2\\phi - \\sinh^2\\phi \\\\ \u0026= 1 \\end{aligned} \\]\nWe can simplify our definition of a normal paravector even further by looking back to Euler's formula and geometric algebra where we saw that for a normal vector \\(\\v{a}\\),\n\\[ e^{\\mathbf{a}\\phi} = \\cosh\\phi + \\mathbf{a}\\sinh\\phi = \\v{A}\\]\nWith this simpler definition of a normal paravector, \\( \\v{A} = e^{\\v{a}\\phi}\\), it becomes trivial to show that is is indeed normal:\n\\[ \\begin{aligned} |\\v{A}|^2 \u0026= \\v{A}\\c{\\v{A}} \\\\ \u0026= e^{\\v{a}\\phi}e^{-\\v{a}\\phi} \\\\ \u0026= 1 \\end{aligned} \\]\nRotating a paravector in space only If we define a spatial rotation using two normalised vectors, \\(\\v{b}\\) and \\(\\v{c}\\), as we did in A Rotation in Space, where the difference in orientation between the two is \\(\\theta\\), so that:\n\\[ \\v{R} = \\mathbf{bc} = e^{\\mathbf{xy}\\theta} \\]\nwe find that we cannot simply multiply a paravector by the rotor to obtain a new paravector, as:\n\\[ \\begin{aligned} \\v{AR} \u0026= \\v{A}e^{\\v{xy}\\theta} \\\\ \u0026= (a_0 + \\v{a})e^{\\v{xy}\\theta} \\\\ \u0026= a_0 e^{\\v{xy}\\theta} + \\v{a}e^{\\v{xy}\\theta} \\\\ \\end{aligned} \\]\nincludes a bivector (\\(\\v{xy}\\)) term (if you expand the first term in the above result). The similar problem arises if we try to simply multiply an arbitrary three-dimensional vector by our rotor. It turns out that a much more general way to apply rotations (and other operations) is use the Clifford conjugate! If we halve the angle between the two vectors defining our rotor, so that:\n\\[ \\v{R} = \\mathbf{bc} = e^{-\\v{xy}\\frac{\\theta}{2}} \\]\nand apply it twice - once from the left as the conjugate and once from the right, we find:\n\\[ \\begin{aligned} \\v{\\c{R}AR} \u0026= e^{\\v{xy}\\frac{\\theta}{2}}(a_0 + \\v{a})e^{-\\v{xy}\\frac{\\theta}{2}} \\\\ \u0026= e^{\\v{xy}\\frac{\\theta}{2}}a_0e^{-\\v{xy}\\frac{\\theta}{2}} + e^{\\v{xy}\\frac{\\theta}{2}}\\v{a}e^{-\\v{xy}\\frac{\\theta}{2}} \\\\ \u0026= a_0 + \\v{a}e^{-\\v{xy}\\theta} \\\\ \\end{aligned} \\]\nThat is, when applying the spatial rotation in this way, the vector component of \\(\\v{A}\\) is rotated by \\(\\theta\\), while the scalar part is not affected at all.\nThe fact that \\(\\v{\\c{R}AR}\\) is a rotation can be verified simply by confirming that the amplitude-squared is unchanged:\n\\[ \\begin{aligned} |\\v{\\c{R}AR}|^2 \u0026= \\v{\\c{R}AR}\\c{\\v{\\c{R}AR}} \\\\ \u0026= \\v{\\c{R}AR}\\v{\\c{R}\\c{A}R} \\\\ \u0026= \\v{A}\\c{\\v{A}} \\end{aligned} \\]\nBut what if, rather than defining a rotation as a normalised combination of a scalar and a bivector, we instead define a rotation as a normalized combination of a scalar and a vector? As we will see, it turns out to be equivalent to the Lorentz transformation of Special Relativity!\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-spacetime/",
	"title": "Rotations in Space-Time",
	"tags": [],
	"description": "",
	"content": "Building on rotations using geometric algebra, in this section we define a two-dimensional space-time multivector and then investigate a rotation in space as well as a rotation in space-time, the result of which is equivalent to the Lorentz transformation.\n A (para)Vector of space and time  So far we have considered operations on vectors only - linear combinations of the two basis vectors, \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Here we will consider a more general combination of a vector together with a scalar component \\(a_0\\) to form a something called a paravector - a scalar value plus a vector value. Furthermore, we will see that this scalar component results in a amplitude-squared metric matching the metric of space-time in Special Relativity. Finally, we will see that we can rotate these space-time paravectors in space without affecting the temporal component, corresponding to changing our frame of reference within a single inertial frame.\n The Lorentz Transformation  In the previous section we saw that a combination of a scalar and a vector, a paravector, \\(\\v{A} = a_0 + \\v{a}\\), has an amplitude-squared with the same metric as that of spacetime and yet can be rotated like a normal vector such that the scalar part is unchanged, while the vector part rotates in space, as \\(\\v{\\c{R}}\\v{A}\\v{R}\\), where the rotor \\(\\v{R}\\) is itself comprised of a scalar and a bivector, \\(\\v{R} = e^{-\\v{xy}\\frac{\\theta}{2}}\\).\nWe also saw that we can define a normalised paravector as \\(\\v{B} = e^{-\\v{b}\\frac{\\phi}{2}}\\), so that \\(\\v{B}\\c{\\v{B}} = 1\\). In this section we will consider rotating the paravector \\(\\v{A}\\) by a normalized paravector and see that the result, \\(\\v{B}\\v{A}\\v{B}\\), is equivalent to the standard Lorentz transformation.\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-spacetime/2d-rotations-in-spacetime/",
	"title": "The Lorentz Transformation",
	"tags": [],
	"description": "",
	"content": "In the previous section we saw that a combination of a scalar and a vector, a paravector, \\(\\v{A} = a_0 + \\v{a}\\), has an amplitude-squared with the same metric as that of spacetime and yet can be rotated like a normal vector such that the scalar part is unchanged, while the vector part rotates in space, as \\(\\v{\\c{R}}\\v{A}\\v{R}\\), where the rotor \\(\\v{R}\\) is itself comprised of a scalar and a bivector, \\(\\v{R} = e^{-\\v{xy}\\frac{\\theta}{2}}\\).\nWe also saw that we can define a normalised paravector as \\(\\v{B} = e^{-\\v{b}\\frac{\\phi}{2}}\\), so that \\(\\v{B}\\c{\\v{B}} = 1\\). In this section we will consider rotating the paravector \\(\\v{A}\\) by a normalized paravector and see that the result, \\(\\v{B}\\v{A}\\v{B}\\), is equivalent to the standard Lorentz transformation.\nA hyperbolic rotation Similar to the spatial rotation, we can see that \\(\\v{A'} = \\v{B}\\v{A}\\v{B}\\) is some type of rotation of \\(\\v{A}\\), as even though we are not using the Clifford conjugation of the rotor for the left-hand multilication, the amplitude squared is still unchanged:\n\\[ \\begin{aligned} \\v{\\c{A'}A'} \u0026= \\c{\\v{B}\\v{A}\\v{B}} \\v{B}\\v{A}\\v{B} \\\\ \u0026= \\c{\\v{B}}\\c{\\v{A}}\\c{\\v{B}} \\v{B}\\v{A}\\v{B} \\\\ \u0026= \\v{\\c{A}A} \\end{aligned} \\]\nEvaluating \\(\\v{A'}\\), we find\n\\[ \\begin{aligned} \\v{A'} \u0026= \\v{B}\\v{A}\\v{B} \\\\ \u0026= e^{-\\v{b}\\frac{\\phi}{2}}(a_0 + \\v{a})e^{-\\v{b}\\frac{\\phi}{2}} \\\\ \u0026= a_0 e^{-\\v{b}\\phi} + e^{-\\v{b}\\frac{\\phi}{2}} \\v{a}e^{-\\v{b}\\frac{\\phi}{2}} \\end{aligned} \\]\nwhich is, in fact...\nThe Lorentz Transformation A vector in the same direction as the rotation If we first look at the case where the vector being rotated is in the same direction as the rotation itself, ie. \\(\\v{a}\\v{b} = \\v{b}\\v{a} = |\\v{a}|\\) and since \\(\\v{b}^2 = 1\\), then \\(\\v{a} = |\\v{a}|\\v{b}\\), we can further simplify the rotation:\n\\( \\begin{aligned} \\v{A'} \u0026= a_0 e^{-\\v{b}\\phi} + e^{-\\v{b}\\frac{\\phi}{2}} \\v{a}e^{-\\v{b}\\frac{\\phi}{2}} \\\\ \u0026= a_0 e^{-\\v{b}\\phi} + \\v{a}e^{-\\v{b}\\phi} \\\\ \u0026= a_0 (\\cosh{\\phi} - \\v{b} \\sinh{\\phi}) + \\v{a}(\\cosh{\\phi} - \\v{b} \\sinh{\\phi}) \\\\ \u0026= (a_0 \\cosh{\\phi} - \\v{ab}\\sinh{\\phi}) + (\\v{a}\\cosh{\\phi} - a_0 \\v{b} \\sinh{\\phi}) \\\\ \u0026= (a_0 \\cosh{\\phi} - |\\v{a}|\\sinh{\\phi}) + \\v{b}(|\\v{a}|\\cosh{\\phi} - a_0 \\sinh{\\phi}) \\\\ \u0026= \\cosh{\\phi}(a_0 - |\\v{a}|\\tanh{\\phi}) + \\v{b}\\cosh{\\phi}(|\\v{a}| - a_0 \\tanh{\\phi}) \\\\ \\end{aligned} \\)  Plotting sinh, cosh and tanh Wikimedia Commons\n  Note that \\(\\tanh\\phi\\) has the property of ranging from -1 to 1 (see figure) and that \\(\\cosh\\phi\\) and \\(\\tanh\\phi\\) can be related by:\n\\[ \\cosh\\phi = \\frac{1}{\\sqrt{1-\\tan^2\\phi}} \\]\nSo with the substitution of \\(v = \\tanh\\phi\\), we're left with the Lorentz transformation of \\(\\v{A} = a_0 + \\v{a}\\):\n\\[ \\v{B}\\v{A}\\v{B} = \\frac{1}{\\sqrt{1-v^2}}(a_0 - |\\v{a}|v) + \\frac{1}{\\sqrt{1-v^2}}(|\\v{a}| - a_0 v)\\v{x} \\]\nA vector perpendicular to the direction of the rotation Next let us look at the case where the vector being rotated is perpendicular to the rotation, ie. \\(\\v{a}\\v{b} = -\\v{b}\\v{a}\\). In this case:\n\\[ \\begin{aligned} \\v{A'} \u0026= a_0 e^{-\\v{b}\\phi} + e^{-\\v{b}\\frac{\\phi}{2}} \\v{a}e^{-\\v{b}\\frac{\\phi}{2}} \\\\ \u0026= a_0 e^{-\\v{b}\\phi} + \\v{a}e^{+\\v{b}\\frac{\\phi}{2}} e^{-\\v{b}\\frac{\\phi}{2}} \\\\ \u0026= a_0 e^{-\\v{b}\\phi} + \\v{a} \\\\ \\end{aligned} \\]\nSo the vector perpendicular to the direction of the rotation is not affected by the transformation, just as with the Lorentz transformation.\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/eulers-formula/",
	"title": "Euler&#39;s Formula",
	"tags": [],
	"description": "",
	"content": "Euler's formula, the most remarkable formula in mathematics, according to Richard Feynman and many others, states that for any real number \\(x\\),\n\\[ e^{ix} = \\cos x + i \\sin x \\]\nIn this section we will derive Euler's formula to get a better understanding of how Euler's formula can be used to describe rotations in space and time, eventually including the Lorentz transformation.\nSkip if you prefer... The derivation of Euler's formula may be a little complicated if you've not touched calculus in a while. Feel free to skip this section for now and go straight to the next section, coming back later as you need.  Finding a function whose derivative is the function itself Euler's formula comes about by first considering a function of \\( x \\), for which the derivative of the function for any value of \\(x\\) has the same value as function itself. That is:\n\\[ \\frac{d}{dx}f(x) = f(x) \\]\nIf we start with a guess of \\( f(x) = 1 + x \\) then we see it is not quite right as\n\\[ \\frac{d}{dx}(1 + x) = 1 \\ne (1 + x) \\]\nbut we can improve it by adding another term\n\\[ f(x) = 1 + x + \\frac{x^2}{2} \\]\nwhich is a little closer in that\n\\[ \\frac{d}{dx}(1 + x + \\frac{x^2}{2}) = 1 + x \\ne f(x) \\]\nbut still not the same. So we improve it again with\n\\[ f(x) = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{2\\times 3}\\]\nand so on, until we end up with an infinite sum of terms:\n\\[ f(x) = \\frac{x^0}{0!} + \\frac{x^1}{1!} + \\frac{x^2}{2!} + \\frac{x^3}{3!} + ... = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} \\]\nWe now have a function satisfying \\( \\frac{d}{dx} f(x) = f(x) \\).\nThe derivative equals the function Take a couple of minutes to see if you can show that the derivative of our function: $$ f(x) = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} $$ is itself.  Solution \nWe can work out the derivative of each term in the infinite sum of terms,\n\\[ \\begin{aligned} \\frac{d}{dx} f(x) \u0026= \\frac{d}{dx} \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{kx^{k-1}}{k!} \\\\ \\end{aligned} \\]\nso we can completely remove the term for \\(k=0\\) (as zero times anything is still zero), and continue:\n\\[ \\begin{aligned} \\frac{d}{dx} f(x) \u0026= \\sum_{k=1}^{\\infty} k\\frac{x^{k-1}}{k!} \\\\ \u0026= \\sum_{k=1}^{\\infty} \\frac{x^{k-1}}{(k - 1)!} \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} \\\\ \u0026= f(x) \\end{aligned} \\]\nDone!\n  Great, but where does \\(e\\) fit in?\nUnderstanding what an exponent is We learn in high school that \\( 3^2 \\times 3 = 3^3 \\) and \\( 2^2 \\times 2 \\times 2 = 2^4 \\) and later that \\( x^2 \\times x^2 = x^4 \\) etc., but the general definition of exponentiation is that when multiplying two numbers with the same base (\\( 3^2 \\times 3^1 \\)), the powers (or exponents) are added (\\( 3^{2+1} = 3^3 \\)):\n\\[ b^m \\times b^n = b^{m + n} \\]\nOur function is an exponential If we take our function which satisfies the property \\(\\frac{d}{dx}f(x) = f(x)\\) we can show that it behaves just like an exponent should, in that:\n  \\( f(m) \\times f(n) = f(m+n) \\) \nTo show this requires quite a bit of re-arranging:\n\\[ \\begin{aligned} f(m) \\times f(n) \u0026= \\sum_{k=0}^{\\infty} \\frac{m^k}{k!} \\times \\sum_{k=0}^{\\infty} \\frac{n^k}{k!} \\\\ \u0026= (\\frac{m^0}{0!} + \\frac{m^1}{1!} + \\frac{m^2}{2!} + \\frac{m^3}{3!} + ...) \\times (\\frac{n^0}{0!} + \\frac{n^1}{1!} + \\frac{n^2}{2!} + \\frac{n^3}{3!} + ...) \\\\ \u0026= 1 + (\\frac{m^1}{1!} + \\frac{n^1}{1!}) + (\\frac{m^2}{2!} + \\frac{m^1 n^1}{1! 1!} + \\frac{n^2}{2!}) + (\\frac{m^3}{3!} + \\frac{m^2 n^1}{2! 1!} + \\frac{m^1 n^2}{1! 2!} + \\frac{n^3}{3!}) ... \\\\ \u0026= 1 + (\\frac{m^1}{1!} + \\frac{n^1}{1!}) + (\\frac{m^2}{2!} + \\frac{2}{2}\\frac{m^1 n^1}{1! 1!} + \\frac{n^2}{2!}) + (\\frac{m^3}{3!} + \\frac{3}{3}\\frac{m^2 n^1}{2! 1!} + \\frac{3}{3}\\frac{m^1 n^2}{1! 2!} + \\frac{n^3}{3!}) ... \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{(m + n)^k}{k!} \\\\ \u0026= f(m + n) \\end{aligned} \\]\n\nAlso similar to an exponent, we can show for our function \\( f(x) \\), that \\(f(0) = 1\\).\nIdentity of f(0) Show that $$f(0) = 1$$ for our function: $$ f(x) = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!} $$ Solution:\n\nSubstituting \\(x=0\\),\n\\[ \\begin{aligned} f(0) \u0026= \\sum_{k=0}^{\\infty} \\frac{0^k}{k!} \\\\ \u0026= \\frac{0^0}{0!} + \\frac{0^1}{1!} + \\frac{0^2}{2!} + ... \\\\ \u0026= 1 + 0 + 0 + ... \\\\ \u0026= 1 \\end{aligned} \\]\nDone!\n  Given that our function \\(f(x)\\) behaves just like an exponent should behave, it must be equivalent to some number to the power of \\(x\\). We can find that number by working out \\(f(1)\\).\nFind f(1) Show that $$f(1) \\approx 2.717$$ Solution:\n\nEvaluating,\n\\[ \\begin{aligned} f(1) \u0026= \\sum_{k=0}^{\\infty} \\frac{1!}{k!} \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\\\ \u0026= \\frac{1}{0!} + \\frac{1}{1!} + \\frac{1}{2!} + \\frac{1}{3!} + \\frac{1}{4!} + \\frac{1}{5!} + ... \\\\ \u0026= 1 + 1 + 0.5 + 0.1667 + 0.0417 + 0.0083... \\\\ \u0026= 2.717 \\end{aligned} \\]\nThe exact value (if we summed to infinity) is assigned a special name in maths, e.\n  So,\n\\[ e^1 = f(1) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\approx 2.71828... \\]\nNow any value of our function can be evaluated simply as a power of \\(e\\):\n\\[ f(3) = e^3 = 2.71828... ^3 = 20.08534...\\]\nDerivatives of exponentials What if we need to work with other exponents of \\(e\\), such as \\(e^{3x}\\)? How do we work out the derivative - how fast \\(e^{3x}\\) is changing as \\(x\\) changes? Your memory may (or may not) tell you that:\n\\[ \\frac{d}{dx} e^{3x} = 3e^{3x} \\]\nor more generally, if \\(g\\) is another function of \\(x\\), that\n\\[ \\frac{d}{dx} e^{g} = \\frac{dg}{dx}e^{g} \\]\nThis is all we need for our tooling here, though we can also see why this is the case given the chain rule for derivatives which says that:\n\\[ \\frac{d}{dx}f(g) = \\frac{dg}{dx} \\times \\frac{d}{d g}f(g) \\]\ntogether with the special property of our function \\(f(x) = e^x\\) that the derivative at any point is equal to the function itself, \\(\\frac{d}{dx}f(x) = f(x)\\).\nEuler's formula Now we have enough background to appreciate the beauty of Euler's formula. Euler, like Roger Cotes before him, noticed that if he evaluated this exponent function with a special type of value, an \u0026quot;imaginary\u0026quot; value whose square is negative, the result is a combination of the trigonometric functions \\(\\cos\\) and \\(sin\\). That is, if we define \\(i\\) such that \\(i^2 = -1\\), and evaluate \\(e^{ix}\\), it conveniently splits into two sums, one of the odd powers and one of the even powers:\n\\[ \\begin{aligned} e^{ix} \u0026= \\sum_{k=0}^{\\infty} \\frac{(ix)^k}{k!} \\\\ \u0026= \\frac{(ix)^0}{0!} + \\frac{(ix)^1}{1!} + \\frac{(ix)^2}{2!} + \\frac{(ix)^3}{3!} + \\frac{(ix)^4}{4!} ... \\\\ \u0026= (1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} ...) + i(\\frac{x^1}{1!} - \\frac{x^3}{3!} + \\frac{x^5}{5!} ...) \\\\ \u0026= \\sum_{k=0}^{\\infty}(-1)^{k}\\frac{x^{2k}}{2k!} + i \\sum_{k=0}^{\\infty}(-1)^k\\frac{x^{2k+1}}{(2k+1)!} \\\\ \u0026= \\cos(x) + i\\sin(x) \\end{aligned} \\]\nThat is, the two separate summations are the actual definitions of \\(\\cos(x)\\) and \\(\\sin(x)\\).\nNotice that, with our introduction to Geometric Algebra, we did not need to invent an imaginary unit \\(i\\) here, as we have already seen that the product of basis vectors \\(\\mathbf{xy}\\) has the same property that \\((\\mathbf{xy})^2 = -1\\), but we will pick up this thread in Euler's formula and Geometric Algebra next.\nDerivatives of cos and sin It's worth noting that we can calculate the derivative of \\(e^{ix}\\) and as a result work out the standard derivations of \\(\\cos\\) and \\(sin\\) that we may have memorized in school or otherwise:\n\\[ \\begin{aligned} \\frac{d}{dx}e^{ix} \u0026= ie^{ix} \\\\ \u0026= i\\cos(x) + i^2\\sin(x) \\\\ \u0026= -\\sin(x) + i\\cos(x) \\end{aligned} \\]\nand so comparing the real parts of \\(e^{ix}\\) and \\(\\frac{d}{dx}e^{ix}\\) show that\n\\[ \\frac{d}{dx}\\cos(x) = -\\sin(x)\\]\nwhile comparing the imaginary parts show that\n\\[ \\frac{d}{dx}\\sin(x) = \\cos(x) \\]\n"
},
{
	"uri": "https://geometry-of-relativity.net/rotations-in-space/eulers-formula-and-geometric-algebra/",
	"title": "Euler&#39;s Formula and Geometric Algebra",
	"tags": [],
	"description": "",
	"content": "While working to understand and derive Euler's formula we introduced an imaginary unit \\(i\\) with the property that \\(i^2 = -1\\). But we've already seen that the product of the two basis vectors has this same property in that \\( (\\v{xy})^2 = -1 \\), though it differs in other properties as we shall see.\nIn this section we will investigate the properties of various rotations in 2D planes using Euler's formula and Geometric Algebra in two dimensional space.\nA rotation in space Rather than introducing the imaginary unit \\(i\\) we could have evaluated the exponent function using the value \\(\\v{xy}\\theta\\) instead. Doing so leads to a similarly periodic result:\n\\[ e^{\\v{xy}\\theta} = \\cos \\theta + \\v{xy}\\sin \\theta \\]\nRather than representing a complex rotation, in this form it represents a rotation in the \\(\\v{xy}\\) plane of our two dimensional space. To see that this is the case, consider multiplying an arbitrary vector \\(\\v{a}\\) by this rotation:\n\\[ \\begin{aligned} \\v{a}e^{\\v{xy}\\theta} \u0026= (\\vcTwo{a})(\\cos \\theta + \\v{xy}\\sin \\theta) \\\\ \u0026= a_x \\cos\\theta \\v{x} + a_x\\v{xxy}\\sin\\theta + a_y\\cos\\theta\\v{y} + a_y\\v{yxy}\\sin\\theta \\\\ \u0026= (a_x \\cos\\theta - a_y\\sin\\theta)\\v{x} + (a_x\\sin\\theta + a_y\\cos\\theta)\\v{y} \\end{aligned} \\]\nwhich matches the normal definition for a two-dimensional rotation around the origin of a point with coordinates \\((a_x, a_y)\\).\nIn fact, the rotor \\(e^{\\v{xy}\\theta}\\) is itself formed by multiplying two normal vectors separated by an angle \\(\\theta\\). We already saw when multiplying two vectors, that:\n\\[ \\v{ab} = a_x b_x + a_y b_y + \\v{xy}(a_x b_y - a_y b_x) \\]\nwhich, if \\(\\v{a}^2 = \\v{b}^2 = 1\\), is equivalent to\n\\[ \\cos \\theta + \\v{xy}\\sin \\theta = e^{\\v{xy}\\theta}\\]\nFor now we need to pause and note a few properties of this rotation \\(e^{\\v{xy}\\theta}\\).\nProperties of spatial rotations First, the exponent differs in sign depending on whether you left or right multiply your vector, so:\n\\[ \\v{x}e^{\\v{xy}\\theta} = e^{-\\v{xy}\\theta} \\v{x} \\\\ \\v{y}e^{\\v{xy}\\theta} = e^{-\\v{xy}\\theta} \\v{y} \\]\nand therefore more generally\n\\[ \\v{a}e^{\\v{xy}\\theta} = e^{-\\v{xy}\\theta}\\v{a} \\]\nThis can be seen by expanding to the trigonometric values as follows:\n\\[ \\begin{aligned} \\v{x}e^{\\v{xy}\\theta} \u0026= \\v{x}\\cos \\theta + \\v{xxy}\\sin \\theta \\\\ \u0026= \\v{x} \\cos \\theta -\\v{xyx}\\sin \\theta \\\\ \u0026= (\\cos \\theta -\\v{xy}\\sin \\theta)\\v{x}\\\\ \u0026= e^{-\\v{xy}\\theta} \\v{x} \\end{aligned} \\]\nNext, the exponent does not differ in sign when left or right multiplying by the pseudo-scalar \\(\\v{xy}\\):\n\\[ \\v{xy}e^{\\v{xy}\\theta} = e^{\\v{xy}\\theta} \\v{xy} \\]\nwhich can be seen again by expanding as per the previous example.\nFinally, it's worth noting that, based on the above definition of \\(e^{\\v{xy}\\theta}\\), both \\(\\cos\\theta\\) and \\(\\sin\\theta\\) can be represented as a combination of exponentials as follows:\n\\[ \\cos\\theta = \\frac{e^{\\v{xy}\\theta} + e^{-\\v{xy}\\theta}}{2} \\]\n\\[ \\sin\\theta = \\frac{e^{\\v{xy}\\theta} - e^{-\\v{xy}\\theta}}{2\\v{xy}} \\]\nProperties of hyperbolic rotations We can also investigate rotations using another exponent, such as \\(\\v{x}\\theta\\), which results in something less familiar, but just as significant when looking at the geometry of relativity.\nIn this case, we need to return to the full definition of our exponential function which we derived with Euler's formula, to see how it behaves:\n\\[ \\begin{aligned} e^{\\v{x}\\theta} \u0026= \\sum_{k=0}^{\\infty} \\frac{(\\v{x}\\theta)^k}{k!} \\\\ \u0026= \\frac{(\\v{x}\\theta)^0}{0!} + \\frac{(\\v{x}\\theta)^1}{1!} + \\frac{(\\v{x}\\theta)^2}{2!} + \\frac{(\\v{x}\\theta)^3}{3!} + ... \\\\ \u0026= \\sum_{k=0}^{\\infty} \\frac{\\theta^{2k}}{2k!} + \\v{x}\\sum_{k=0}^{\\infty} \\frac{\\theta^{2k + 1}}{(2k +1)!} \\end{aligned} \\]\nThe two summations in the last line are one way to define the hyperbolic functions \\(\\cosh\\theta\\) and \\(\\sinh\\theta\\) respectively, so that\n\\[ e^{\\v{x}\\theta} = \\cosh\\theta + \\v{x}\\sinh\\theta \\]\nWe will see later how this hyperbolic rotation becomes important for two-dimensional rotations in space-time but for now, a few analogous but different properties to note, all of which can be seen by expanding \\(e^{\\v{x}\\theta}\\) to the equivalent trigonometric values:\n\\[\\v{x}e^{\\v{x}\\theta} = e^{\\v{x}\\theta}\\v{x}\\]\n\\[\\v{y}e^{\\v{x}\\theta} = e^{-\\v{x}\\theta}\\v{y}\\]\n\\[\\v{x}e^{\\v{y}\\theta} = e^{-\\v{y}\\theta}\\v{x}\\]\n\\[\\v{y}e^{\\v{y}\\theta} = e^{\\v{y}\\theta}\\v{y}\\]\n\\[\\v{xy}e^{\\v{x}\\theta} = e^{-\\v{x}\\theta}\\v{xy}\\]\n\\[\\v{xy}e^{\\v{y}\\theta} = e^{-\\v{y}\\theta}\\v{xy}\\]\n\\[ \\cosh\\theta = \\frac{e^{\\v{x}\\theta} + e^{-\\v{x}\\theta}}{2} \\]\n\\[ \\sinh\\theta = \\frac{e^{\\v{x}\\theta} - e^{-\\v{x}\\theta}}{2\\v{x}} \\]\n"
},
{
	"uri": "https://geometry-of-relativity.net/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://geometry-of-relativity.net/credits/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": "Contributors In addition to the contributors, like most fans of Geometric Algebra out there, I would never have heard of Geometric Algebra without the tireless work of David Hestines to promote it during his whole career, which influenced the work of Dr Chris Doran and Professor Anthony Lasenby at the Cambridge Geometric Algebra group.\nThanks to them  for helping to spread the love of Geometric Algebra and having fun playing with its application in physics. If you are interested you can help others learn by submitting a Github pull request and your contributions will be listed here automatically! I need help creating visualisations, correcting and improving the text, as well as adding activities where helpful.\n.ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start }\n.ghContributors \u0026gt; div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors \u0026gt; div label{ padding-left: 4px ; } .ghContributors \u0026gt; div span{ font-size: x-small; padding-left: 4px ; }\n \n\u0026lt;img src=\u0026#34;https://avatars0.githubusercontent.com/u/497518?v=4\u0026#34; class=\u0026#34;inline\u0026#34; width=\u0026#34;32\u0026#34; height=\u0026#34;32\u0026#34; style=\u0026#34;height: 32px;height: 32px;margin-bottom:.25em; vertical-align:middle; \u0026#34;\u0026gt; \u0026lt;label\u0026gt;\u0026lt;a href=\u0026#34;https://github.com/absoludity\u0026#34;\u0026gt;@absoludity\u0026lt;/a\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;span class=\u0026#34;contributions\u0026#34;\u0026gt;8 commits\u0026lt;/span\u0026gt; \n\n"
},
{
	"uri": "https://geometry-of-relativity.net/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]